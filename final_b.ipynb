{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Files:\n",
    "\n",
    "NOAA Data\n",
    "from ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ :\n",
    "1. ghcnd-stations.txt\n",
    "2. ghcnd-states.txt\n",
    "3. ghcnd_all.tar.gz  - for this zipped file you will need a program like winrar/7zip\n",
    "\n",
    "FRA Data\n",
    "from https://data.transportation.gov/Railroads/Highway-Rail-Grade-Crossing-Accident-Data/7wn6-i5b9 : \n",
    "1. https://data.transportation.gov/api/views/7wn6-i5b9/rows.csv?accessType=DOWNLOAD&bom=true&format=true\n",
    "\n",
    "Cities Data\n",
    "from https://github.com/kelvins/US-Cities-Database : \n",
    "1. https://github.com/kelvins/US-Cities-Database/blob/main/csv/us_cities.csv\n",
    "\n",
    "-------\n",
    "Additional Files\n",
    "\n",
    "Weather Events Data\n",
    "from https://www.kaggle.com/sobhanmoosavi/us-weather-events :\n",
    "1. https://www.kaggle.com/sobhanmoosavi/us-weather-events/download\n",
    "\n",
    "Safety Events Data \n",
    "from https://www.transit.dot.gov/ntd/data-product/safety-security-major-only-time-series-data :\n",
    "1. https://data.transportation.gov/Public-Transit/Major-Safety-Events/9ivb-8ae9\n",
    "\n",
    "## Instructions\n",
    "\n",
    "# IMPORTANT - You will need ~40 GB of HDD or SSD space for NOAA Data, as well as 2 hours (?) for downloading + extracting noaa data \n",
    "# (you dont have to be at the computer for most of that time)\n",
    "\n",
    "\n",
    "1. Download all files  and move them to the directory you plan to work in (working directory / wd)\n",
    "2. select the file ghcnd_all.tar.gz and open it with your unzipping tool (I used WinRAR), it will take a multiple minutes to load due to the size (~120,000 files)\n",
    "3. once everything is loaded, you will see a text file ghcnd-version.txt, and folder ghcnd_all.  \n",
    "\n",
    "you have two options for the next part, extracting all data or extracting only US data.  Both take a long time, only getting US data is a slightly faster but slightly more effort.\n",
    "\n",
    "extract all:\n",
    "4. Select the folder and extract to your current working directory (or another directory of your choice).  You will need ~40 GB  of space to be safe, I used an external harddrive.  Extraction will take a while, so go grab a snack or watch an episode of something. \n",
    "\n",
    "extract US only:\n",
    "4. Select the folder and open it in WinRAR/7Zip, it will take a minute to load all the files inside.  Sort the files by name. Scroll down to 'US'. Select all files beginning with 'US'. and extract them to a new folder 'ghcnd_all' in your working directory (wd/ghcnd_all). Extraction still takes a while, but this is faster than getting all 120k files. \n",
    "\n",
    "last step: \n",
    "check the imports below, pip install any that are missing from your pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure you have at least: pandas 1.2.3 , numpy 1.21.x ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alabaster==0.7.12\n",
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.10.0\n",
      "anaconda-project==0.8.3\n",
      "asn1crypto==1.0.1\n",
      "astroid==2.3.1\n",
      "astropy==3.2.1\n",
      "atomicwrites==1.3.0\n",
      "attrs==19.2.0\n",
      "Babel==2.7.0\n",
      "backcall==0.1.0\n",
      "backports.functools-lru-cache==1.5\n",
      "backports.os==0.1.1\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "beautifulsoup4==4.8.0\n",
      "bitarray==1.0.1\n",
      "bkcharts==0.2\n",
      "bleach==3.1.0\n",
      "bokeh==1.3.4\n",
      "boto==2.49.0\n",
      "Bottleneck==1.2.1\n",
      "certifi==2019.9.11\n",
      "cffi==1.12.3\n",
      "chardet==3.0.4\n",
      "Click==7.0\n",
      "cloudpickle==1.2.2\n",
      "clyent==1.2.2\n",
      "colorama==0.4.1\n",
      "comtypes==1.1.7\n",
      "conda==4.10.3\n",
      "conda-build==3.18.9\n",
      "conda-package-handling==1.6.0\n",
      "conda-verify==3.4.2\n",
      "contextlib2==0.6.0\n",
      "cryptography==2.7\n",
      "cycler==0.10.0\n",
      "Cython==0.29.13\n",
      "cytoolz==0.10.0\n",
      "dask==2.5.2\n",
      "decorator==4.4.0\n",
      "defusedxml==0.6.0\n",
      "distributed==2.5.2\n",
      "docutils==0.15.2\n",
      "entrypoints==0.3\n",
      "et-xmlfile==1.0.1\n",
      "fastcache==1.1.0\n",
      "filelock==3.0.12\n",
      "Flask==1.1.1\n",
      "fonttools==4.28.3\n",
      "fsspec==0.5.2\n",
      "future==0.17.1\n",
      "gevent==1.4.0\n",
      "glob2==0.7\n",
      "greenlet==0.4.15\n",
      "gym==0.17.1\n",
      "h5py==2.9.0\n",
      "HeapDict==1.0.1\n",
      "html5lib==1.0.1\n",
      "idna==2.8\n",
      "imageio==2.6.0\n",
      "imagesize==1.1.0\n",
      "importlib-metadata==0.23\n",
      "ipykernel==5.1.2\n",
      "ipython==7.8.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "isort==4.3.21\n",
      "itsdangerous==1.1.0\n",
      "jdcal==1.4.1\n",
      "jedi==0.15.1\n",
      "Jinja2==2.10.3\n",
      "joblib==0.13.2\n",
      "json5==0.8.5\n",
      "jsonschema==3.0.2\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.3.3\n",
      "jupyter-console==6.0.0\n",
      "jupyter-core==4.5.0\n",
      "jupyterlab==1.1.4\n",
      "jupyterlab-server==1.0.6\n",
      "keyring==18.0.0\n",
      "kiwisolver==1.1.0\n",
      "lazy-object-proxy==1.4.2\n",
      "libarchive-c==2.8\n",
      "llvmlite==0.29.0\n",
      "locket==0.2.0\n",
      "lxml==4.4.1\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.5.1\n",
      "mccabe==0.6.1\n",
      "menuinst==1.4.16\n",
      "mistune==0.8.4\n",
      "mkl-fft==1.0.14\n",
      "mkl-random==1.1.0\n",
      "mkl-service==2.3.0\n",
      "mock==3.0.5\n",
      "more-itertools==7.2.0\n",
      "mpmath==1.1.0\n",
      "msgpack==0.6.1\n",
      "multipledispatch==0.6.0\n",
      "mysql-connector-python==8.0.23\n",
      "navigator-updater==0.2.1\n",
      "nbconvert==5.6.0\n",
      "nbformat==4.4.0\n",
      "networkx==2.3\n",
      "nltk==3.4.5\n",
      "noaa-weather==0.1\n",
      "nose==1.3.7\n",
      "notebook==6.0.1\n",
      "numba==0.45.1\n",
      "numexpr==2.7.0\n",
      "numpy==1.21.4\n",
      "numpydoc==0.9.1\n",
      "olefile==0.46\n",
      "openpyxl==3.0.0\n",
      "packaging==21.3\n",
      "pandas==1.3.5\n",
      "pandocfilters==1.4.2\n",
      "parso==0.5.1\n",
      "partd==1.0.0\n",
      "path.py==12.0.1\n",
      "pathlib2==2.3.5\n",
      "patsy==0.5.1\n",
      "pep8==1.7.1\n",
      "pickleshare==0.7.5\n",
      "Pillow==6.2.0\n",
      "pkginfo==1.5.0.1\n",
      "pluggy==0.13.0\n",
      "ply==3.11\n",
      "prometheus-client==0.7.1\n",
      "prompt-toolkit==2.0.10\n",
      "psutil==5.6.3\n",
      "py==1.8.0\n",
      "pycodestyle==2.5.0\n",
      "pycosat==0.6.3\n",
      "pycparser==2.19\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.43.0.3\n",
      "pydot==1.4.1\n",
      "pyflakes==2.1.1\n",
      "pygame==1.9.6\n",
      "pyglet==1.5.0\n",
      "Pygments==2.4.2\n",
      "pylint==2.4.2\n",
      "pyodbc==4.0.27\n",
      "pyOpenSSL==19.0.0\n",
      "pyparsing==2.4.2\n",
      "pyreadline==2.1\n",
      "pyrsistent==0.15.4\n",
      "PySocks==1.7.1\n",
      "pytest==5.2.1\n",
      "pytest-arraydiff==0.3\n",
      "pytest-astropy==0.5.0\n",
      "pytest-doctestplus==0.4.0\n",
      "pytest-openfiles==0.4.0\n",
      "pytest-remotedata==0.3.2\n",
      "python-dateutil==2.8.0\n",
      "pytz==2019.3\n",
      "PyWavelets==1.0.3\n",
      "pywin32==223\n",
      "pywinpty==0.5.5\n",
      "PyYAML==5.1.2\n",
      "pyzmq==18.1.0\n",
      "QtAwesome==0.6.0\n",
      "qtconsole==4.5.5\n",
      "QtPy==1.9.0\n",
      "requests==2.22.0\n",
      "rope==0.14.0\n",
      "ruamel-yaml==0.15.46\n",
      "scikit-image==0.15.0\n",
      "scikit-learn==0.21.3\n",
      "scipy==1.3.1\n",
      "seaborn==0.9.0\n",
      "Send2Trash==1.5.0\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.12.0\n",
      "snowballstemmer==2.0.0\n",
      "sortedcollections==1.1.2\n",
      "sortedcontainers==2.1.0\n",
      "soupsieve==1.9.3\n",
      "Sphinx==2.2.0\n",
      "sphinxcontrib-applehelp==1.0.1\n",
      "sphinxcontrib-devhelp==1.0.1\n",
      "sphinxcontrib-htmlhelp==1.0.2\n",
      "sphinxcontrib-jsmath==1.0.1\n",
      "sphinxcontrib-qthelp==1.0.2\n",
      "sphinxcontrib-serializinghtml==1.1.3\n",
      "sphinxcontrib-websupport==1.1.2\n",
      "spyder==3.3.6\n",
      "spyder-kernels==0.5.2\n",
      "SQLAlchemy==1.3.9\n",
      "statsmodels==0.10.1\n",
      "sympy==1.4\n",
      "tables==3.5.2\n",
      "tblib==1.4.0\n",
      "terminado==0.8.2\n",
      "testpath==0.4.2\n",
      "toolz==0.10.0\n",
      "tornado==6.0.3\n",
      "tqdm==4.36.1\n",
      "traitlets==4.3.3\n",
      "unicodecsv==0.14.1\n",
      "urllib3==1.24.2\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.16.0\n",
      "widgetsnbextension==3.5.1\n",
      "win-inet-pton==1.1.0\n",
      "win-unicode-console==0.5\n",
      "wincertstore==0.2\n",
      "wrapt==1.11.2\n",
      "xlrd==1.2.0\n",
      "XlsxWriter==1.2.1\n",
      "xlwings==0.15.10\n",
      "xlwt==1.3.0\n",
      "xmltodict==0.12.0\n",
      "zict==1.0.0\n",
      "zipp==0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could not generate requirement for distribution -umpy 1.16.5 (c:\\users\\skurai\\anaconda3\\lib\\site-packages): Parse error at \"'-umpy==1'\": Expected W:(abcd...)\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --user pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --user numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skurai\\practicum\\final\n"
     ]
    }
   ],
   "source": [
    "# get current working directory\n",
    "os.chdir('C:/Users/Skurai/practicum/final')\n",
    "wd = os.getcwd()\n",
    "print(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "We begin with NOAA because it is the most complex. Before we can use NOAA daily data, we have to know which daily files we want to investigate. There are so many files we may not be able to read them all in, so we must restrict the files that we look at by first checking which files will be relevant.  We do this by mapping NOAA Stations to the FRA (or other) dataset(s).  We will only pull daily weather observations for stations that map to other dataset(s).  In this way we reduce the amount of data stored in memory to a workable amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Dailies part 1: NOAA Stations\n",
    "\n",
    "use the function below to create a dataframe containing station information from the ghcnd_stations.txt file.  \n",
    "the only input is your working directory (wd), your ghcnd_stations.txt file must be saved there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noaa_stations_data(wd):\n",
    "    \"\"\"\n",
    "    This function will load the weather station dataset from NOAA\n",
    "    this is used for mapping to rail crossing locations.\n",
    "    \n",
    "    Contains weather station reference data\n",
    "    \n",
    "    Source: ghcnd-stations.txt from NOAA ftp\n",
    "    Input: wd - working directory\n",
    "    Output: stations_df - dataframe of NOAA stations\n",
    "    \"\"\"\n",
    "    f = open(os.path.join(wd,\"ghcnd-stations.txt\"),\"r\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    # columns in the station file\n",
    "    colnames = ['ID', 'LAT', 'LON', 'ELEV', 'STATE', 'NAME', 'GSN', 'HCNCRN', 'WMOID']\n",
    "    stationlist = []\n",
    "\n",
    "    # initialize dataframe with correct columns\n",
    "    stations_df = pd.DataFrame(columns = colnames)\n",
    "\n",
    "    # iterate through stations and add them to our collection of stations if they are in the US\n",
    "    for line in lines:\n",
    "        # first 2 characters are the country code , we only care about us stations\n",
    "        if line[0:2] == 'US':\n",
    "\n",
    "            # the description of the file seemed slightly off, i tested and found these column numbers to work best\n",
    "            row = {\"ID\": line[0:11].upper(),\n",
    "                    \"LAT\": float(line[13:20]),\n",
    "                    \"LON\": float(line[21:30]),\n",
    "                    \"ELEV\": float(line[31:37]),\n",
    "                    \"STATE\": line[38:40],\n",
    "                    \"NAME\": line[41:71],\n",
    "                    \"GSN\": line[72:75],\n",
    "                    \"HCNCRN\": line[76:79],\n",
    "                    \"WMOID\": line[80:85]\n",
    "                   }\n",
    "            stationlist.append(row)\n",
    "        else:\n",
    "            pass\n",
    "    stations_df = stations_df.append(stationlist)\n",
    "    f.close()\n",
    "    \n",
    "    return stations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = load_noaa_stations_data(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Dailies part 2: NOAA States\n",
    "\n",
    "use the function below to create a dataframe containing station+state information from the ghcnd_states.txt file.  \n",
    "the only input is your working directory (wd), your ghcnd_states.txt file must be saved there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noaa_states_data(wd):\n",
    "    \"\"\"\n",
    "    This function will load the state dataset from NOAA\n",
    "    this is used for mapping to rail crossing locations.\n",
    "    \n",
    "    Contains state reference data\n",
    "    \n",
    "    Source: ghcnd-states.txt from NOAA ftp\n",
    "    Input: wd - working directory\n",
    "    Output: states_df - dataframe of NOAA states\n",
    "    \"\"\"\n",
    "    # read in states dataset to supplement weather stations data\n",
    "    f = open(os.path.join(wd,\"ghcnd-states.txt\"),\"r\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    colnames = ['CODE', 'NAME']\n",
    "\n",
    "    # create dataframe of state data\n",
    "    states_df = pd.DataFrame(columns=colnames)\n",
    "    for line in lines:\n",
    "        modline = line.strip('\\n')\n",
    "        data = {'CODE': line[0:2],\n",
    "                \"NAME\": modline[3:50]\n",
    "               }\n",
    "        states_df = states_df.append(data, ignore_index=True)    \n",
    "\n",
    "    f.close()\n",
    "    return states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_df = load_noaa_states_data(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Dailies part 3: NOAA Stations+States\n",
    "\n",
    "use the function below to combine the two dataframes, while also adding the engineered feature 'wcoordinateID'.  \n",
    "'wcoordinateID' will be used to join this dataset with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_noaa_refdata(stations_df, states_df):\n",
    "    \"\"\"\n",
    "    This function will merge the NOAA refdata\n",
    "    this is used for mapping to rail crossing locations.\n",
    "    \n",
    "    Contains state & station reference data\n",
    "    \n",
    "    Input: stations_df, states_df\n",
    "    Output: stations_plus_df - dataframe of NOAA refdata\n",
    "    \"\"\"\n",
    "    \n",
    "    # add state data to the stations dataset\n",
    "    station_plus_df = stations_df.join(states_df.set_index('CODE'), on='STATE', rsuffix='_STATE')\n",
    "\n",
    "    # create our key feature: coordinateID (wcoordinateID for weather)\n",
    "    # round latitude & longitude to 1 decimal, combine them in a tuple (lat, lon)\n",
    "    station_plus_df['wcoordinateID'] = list(zip(round(station_plus_df['LAT'],1),round(station_plus_df['LON'],1)))\n",
    "    station_plus_df = station_plus_df[['ID','ELEV','wcoordinateID']]\n",
    "    \n",
    "    return station_plus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_plus_df = merge_noaa_refdata(stations_df, states_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Dailies part 4: Cities Data\n",
    "\n",
    "As mentioned above, we need to load in FRA data and join it with the station data before we pull in the daily observations.  \n",
    "There is no good candidate for joining FRA and NOAA data, so we must engineer a 'coordinateID' for the FRA dataset, creating a candidate to merge on.\n",
    "to add 'coordinateID' to the FRA data, we first need the city data, so that is the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_us_cities_data(wd):\n",
    "    \"\"\"\n",
    "    This function will load cities data which will \n",
    "    be used to attach coordinateID to other datasets \n",
    "    which only have city or county level data.\n",
    "    Also derives county locations.\n",
    "    \n",
    "    Input: wd - working directory\n",
    "    Output: grouped_meancounties_df\n",
    "    \"\"\"\n",
    "    cities_df = pd.read_csv(os.path.join(wd,\"us_cities.csv\"))\n",
    "    \n",
    "    # standardize county and state, city is not populated for all events.\n",
    "    # one change to approach would be to include all cities + the grouped mean of each county\n",
    "    cities_df['County'] = cities_df['COUNTY'].str.upper()\n",
    "    cities_df['State'] = cities_df['STATE_NAME'].str.upper()\n",
    "    return cities_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = load_us_cities_data(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_meancounties(cities):\n",
    "    # subset of data that we care about, lat+lon to make coordinateID, county, state, state code to merge on\n",
    "    counties = cities[['County','State','LATITUDE','LONGITUDE','STATE_CODE']]\n",
    "    grouped_counties = counties.groupby(['State','County'])\n",
    "    grouped_meancounties_df = grouped_counties.mean()\n",
    "    grouped_meancounties_df = grouped_meancounties_df.reset_index()\n",
    "    grouped_meancounties_df['wcoordinateID'] = list(zip(round(grouped_meancounties_df['LATITUDE'],1),round(grouped_meancounties_df['LONGITUDE'],1)))\n",
    "    \n",
    "    return grouped_meancounties_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_meancounties_df = grouped_meancounties(cities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Dailies part 5: FRA Data\n",
    "\n",
    "As mentioned above, we need to load in FRA data and join it with the station data before we pull in the daily observations.  \n",
    "Run the function below to create a dataframe from the FRA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rail_crossing_data(wd, grouped_meancounties_df):\n",
    "    \"\"\"\n",
    "    This function will load data for rail crossings\n",
    "    which will be used for instances for model training.\n",
    "    Will also be used to limit weather station observations\n",
    "    \n",
    "    Input: wd - working directory, grouped_meancounties_df - location base data\n",
    "    Output: rail_city_df\n",
    "    \"\"\"\n",
    "    railcrossing_df = pd.read_csv(os.path.join(wd,\"Highway-Rail_Grade_Crossing_Accident_Data.csv\"))\n",
    "    \n",
    "    # gather the fields necessary for coordinateID, as well as any  fields you want for analysis later \n",
    "    # change to approache what fields we include in refined_rr\n",
    "    refined_rr_df = railcrossing_df #[['Incident Number','Date','County Name', 'State Name']]\n",
    "\n",
    "    # drop any incident without a date\n",
    "    refined_rr_df = refined_rr_df.dropna(subset=['Date'])\n",
    "\n",
    "    # create our feature incident date, which is an integer with format: yyyymmdd \n",
    "    incident_date = refined_rr_df['Date'].str.split(' ', expand=True)\n",
    "    incident_date = incident_date[0].str.split('/', expand=True)\n",
    "    refined_rr_df['incident_date'] = (incident_date[2].astype(int) * 10000) + (incident_date[0].astype(int) * 100) + (incident_date[1].astype(int) * 1)\n",
    "\n",
    "    # merge accident data with city/county data to add coordinateID to each accident.\n",
    "    merg_rail_city_df = refined_rr_df.merge(grouped_meancounties_df, how='inner', left_on=['County Name','State Name'], right_on=['County','State'])\n",
    "    print(\"Shape of merged unfiltered rail_city dataset:  {}\".format(merg_rail_city_df.shape))\n",
    "    merg_rail_city_df = merg_rail_city_df[merg_rail_city_df['incident_date'] > 20160000]\n",
    "    merg_rail_city_df = merg_rail_city_df[['Grade Crossing ID', \n",
    "                                           'Maintenance Parent Railroad Code', \n",
    "                                           'Incident Number',\n",
    "                                           'Crossing Illuminated',\n",
    "                                           'Railroad Type',\n",
    "                                           'Track Type Code', \n",
    "                                           'Number of Locomotive Units',\n",
    "                                           'Number of Cars',\n",
    "                                           'incident_date', \n",
    "                                           'wcoordinateID', \n",
    "                                           'State', \n",
    "                                           'County'\n",
    "                                          ]]\n",
    "    print(\"Shape of merged filtered rail_city dataset:  {}\".format(merg_rail_city_df.shape))\n",
    "    \n",
    "    return merg_rail_city_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DtypeWarning: Columns (6,7,8,25,26,38,42,50,51,52,53,59,64,69,87,88,89,95,97,98,99,100,101,106,126,127,128,129,139,147,148,149,150,151,152) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged unfiltered rail_city dataset:  (231891, 165)\n",
      "Shape of merged filtered rail_city dataset:  (11412, 12)\n"
     ]
    }
   ],
   "source": [
    "merg_rail_city_df = load_rail_crossing_data(wd, grouped_meancounties_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRA Manipulation \n",
    "It may make sense to do as much manipulation of the FRA Data as you can (restricting state/year/etc.) before you merge with the NOAA data. took your preprocessing code and threw it in here for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_rail(merg_rail_city_df):\n",
    "    \"\"\"\n",
    "    This function will preprocess rail df.\n",
    "    Focus is on datetime formats\n",
    "    \n",
    "    input: merg_rail_city_df\n",
    "    output: enriched merg_rail_city_df\n",
    "    \"\"\"\n",
    "    \n",
    "    merg_rail_city_df['incident_datetime'] = pd.to_datetime(merg_rail_city_df['incident_date'], format='%Y%m%d')\n",
    "    merg_rail_city_df['incident_year'] = merg_rail_city_df['incident_datetime'].dt.year\n",
    "    merg_rail_city_df['incident_month'] = merg_rail_city_df['incident_datetime'].dt.month\n",
    "    merg_rail_city_df['incident_year_month'] = merg_rail_city_df['incident_year'].astype(str) + '_' + merg_rail_city_df['incident_month'].astype(str)\n",
    "    \n",
    "    return merg_rail_city_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_rail_city_df = pre_process_rail(merg_rail_city_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRA Merged Stations\n",
    "Use the code below to get the list of relevant weather stations IDs.  \n",
    "the name of each daily file contains the weather station ID, so the list will be the filter used to prevent loading irrelevant daily files from NOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of weather stations - only those that share coordinateID with an accident\n",
    "merged_stations_incidents_df = station_plus_df.merge(merg_rail_city_df,left_on='wcoordinateID', right_on='wcoordinateID', how='inner')\n",
    "\n",
    "# filter by state \n",
    "target_states = ['NEW JERSEY','NEW YORK','PENNSYLVANIA','CONNECTICUT','DELAWARE','MARYLAND','MASSACHUSETTS','NEW HAMPSHIRE','VIRGINIA']\n",
    "target_state_codes = ['NJ', 'NY', 'PA', 'CT', 'DE', 'MD', 'MA', 'NH', 'VA']\n",
    "statefiltered_stations_incidents_df = merged_stations_incidents_df[merged_stations_incidents_df['State'].isin(target_states)]\n",
    "\n",
    "# filter by year\n",
    "yearstatefiltered_stations_incidents_df = statefiltered_stations_incidents_df[statefiltered_stations_incidents_df['incident_year'].isin([2015,2016,2017,2018,2019,2020,2021])]\n",
    "\n",
    "# save a list of the station IDs that were included in the merged dataframe\n",
    "incident_stations = [x.upper() for x in yearstatefiltered_stations_incidents_df['ID'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    }
   ],
   "source": [
    "print(len(incident_stations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading NOAA Daily Data\n",
    "\n",
    "now that we have the incident stations list, we can load daily noaa data using the function below.  \n",
    "## you will need to edit line 41 so that it points to where your daily files are located "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noaa_dailies(wd):\n",
    "    '''\n",
    "    prior attempts to include this data failed because ghcnd_ghc.tar.gz is too large.\n",
    "    by limiting the number of stations included to only those where incidents occurred,\n",
    "    and by limiting the observation years from each station, we can reduce the amount of\n",
    "    memory required to process this data\n",
    "\n",
    "    '''\n",
    "    # with assistance from \n",
    "    # https://stackoverflow.com/questions/62165172/convert-dly-files-to-csv-using-python\n",
    "    # fields as given by the spec\n",
    "    \n",
    "    fields = [\n",
    "        [\"ID\", 1, 11],\n",
    "        [\"YEAR\", 12, 15],\n",
    "        [\"MONTH\", 16, 17],\n",
    "        [\"ELEMENT\", 18, 21]]\n",
    "\n",
    "    offset = 22\n",
    "\n",
    "    for value in range(1, 32):\n",
    "        fields.append((f\"VALUE{value}\", offset,     offset + 4))\n",
    "        fields.append((f\"MFLAG{value}\", offset + 5, offset + 5))\n",
    "        fields.append((f\"QFLAG{value}\", offset + 6, offset + 6))\n",
    "        fields.append((f\"SFLAG{value}\", offset + 7, offset + 7))\n",
    "        offset += 8\n",
    "\n",
    "    # Modify fields to use Python numbering\n",
    "    fields = [[var, start - 1, end] for var, start, end in fields]\n",
    "    fieldnames = [var for var, start, end in fields]\n",
    "\n",
    "\n",
    "    # the goal of this code is to make 1 file TOTAL from many (originally 1 per station)\n",
    "\n",
    "    # enter where you want a csv saved - it will be many Gigs\n",
    "    csv_filename = wd+'\\\\noaa_relevant_dailies.csv'\n",
    "\n",
    "    with open(csv_filename, 'w', newline='') as f_csv:\n",
    "\n",
    "        # glob.glob should aim at the folder where you extracted all the daily files, wd/ghcnd_all - do not forget to include '\\*.dly'\n",
    "        for dly_filename in glob.glob(r'F:\\weather\\ghcnd_all\\*.dly', recursive=True):\n",
    "            path, name = os.path.split(dly_filename)\n",
    "            station = name[:-4].upper()\n",
    "            if station in incident_stations:\n",
    "                # you could replace this with adding to a dataframe or something else, but i am running out of brain power.\n",
    "                with open(dly_filename, newline='') as f_dly:\n",
    "                    spamwriter  = csv.writer(f_csv)\n",
    "                    spamwriter.writerow(fieldnames) \n",
    "\n",
    "                    for line in f_dly:\n",
    "                        row = [line[start:end].strip() for var, start, end in fields]\n",
    "                        year = int(row[1])\n",
    "\n",
    "                        # important check to save memory, only add recent observations\n",
    "                        if year > 2014:\n",
    "                            spamwriter.writerow(row)\n",
    "\n",
    "            \n",
    "                \n",
    "                \n",
    "# end product is a csv with us weather station data.  needs more cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_noaa_dailies(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning NOAA Daily Data\n",
    "\n",
    "previous function took all the relevant .dly files and combined them into a single .csv  \n",
    "now we will work with the csv to further clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_noaa_dailies(wd):\n",
    "    df = pd.read_csv(os.path.join(wd,\"noaa_relevant_dailies.csv\"))\n",
    "    # we added a header row for every file, but we only need 1 header row. remove the others:\n",
    "    df = df[df['YEAR'] != 'YEAR']\n",
    "\n",
    "    # month and year had some strings and some ints. lets standardize\n",
    "    df['YEAR'] = pd.to_numeric(df['YEAR'])\n",
    "    df['MONTH'] = pd.to_numeric(df['MONTH'])\n",
    "\n",
    "\n",
    "    # base for transposed data\n",
    "    base = pd.DataFrame(columns=['ID','YEAR','MONTH','ELEMENT','VALUE', 'MFLAG', 'QFLAG', 'SFLAG'])\n",
    "\n",
    "    # loop through all days to partially transpose the file (day cols -> rows)\n",
    "    for i in range(1,32):\n",
    "        colnames = [f'VALUE{i}', f'MFLAG{i}', f'QFLAG{i}', f'SFLAG{i}']\n",
    "        newcolnames = ['VALUE', 'MFLAG', 'QFLAG', 'SFLAG']\n",
    "        col_order = ['ID','YEAR','MONTH','DAY','ELEMENT', colnames[0], colnames[1], colnames[2], colnames[3]]\n",
    "\n",
    "        df_new = df[['ID','YEAR','MONTH','ELEMENT', colnames[0], colnames[1], colnames[2], colnames[3]]]\n",
    "        df_new['DAY'] = i\n",
    "        df_new = df_new[col_order]\n",
    "        df_new = df_new.rename(columns={colnames[0]:newcolnames[0], colnames[1]:newcolnames[1], colnames[2]:newcolnames[2], colnames[3]:newcolnames[3]})\n",
    "        base = pd.concat([base, df_new], sort=False)\n",
    "\n",
    "\n",
    "    newcsv = base[['ID','YEAR','MONTH','DAY','ELEMENT','VALUE','MFLAG','QFLAG','SFLAG']]\n",
    "\n",
    "    daily_station_coordinates = station_plus_df[['ID','wcoordinateID']]\n",
    "    daily_final = newcsv.merge(daily_station_coordinates, left_on='ID', right_on='ID')\n",
    "    daily_final.to_csv('final_daily_observations.csv')\n",
    "    print('cleaning complete.  final shape: ', daily_final.shape, '.  reload directory to see file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning complete.  final shape:  (2099351, 10) .  reload directory to see file\n"
     ]
    }
   ],
   "source": [
    "clean_noaa_dailies(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Event Data \n",
    "One may load and work with Kaggle weather event dataset using the code below, which includes added 'wcoordinateID' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weatherEvents(wd):\n",
    "    '''\n",
    "    This data may not actually be used by us if we like the daily data\n",
    "    or possibly we will use both somehow\n",
    "    '''\n",
    "    w_events = pd.read_csv('WeatherEvents_Jan2016-Dec2020.csv')\n",
    "\n",
    "    # it looks like 'Severe' and 'Heavy' are most extreme, so filter to these \n",
    "    extreme_severities = ['Severe', 'Heavy']\n",
    "    extreme_w_events = w_events[w_events['Severity'].isin(extreme_severities)]\n",
    "    print(w_events.shape, extreme_w_events.shape)\n",
    "    print(f'extreme events represent about {round(100*(extreme_w_events.shape[0]/w_events.shape[0]),2)} percent of all events in the original data')\n",
    "\n",
    "    # add coordinateID\n",
    "    extreme_w_events['wcoordinateID'] = list(zip(round(extreme_w_events['LocationLat'],1),round(extreme_w_events['LocationLng'],1)))\n",
    "\n",
    "    # calculate start and end dates as integers yyyymmdd\n",
    "    w_events_date_start = extreme_w_events['StartTime(UTC)'].str.split(' ', expand=True)\n",
    "    w_events_date_start = w_events_date_start[0].str.split('-', expand=True)\n",
    "    extreme_w_events['event_start_dt'] = (w_events_date_start[0].astype(int) * 10000) + (w_events_date_start[1].astype(int) * 100) + (w_events_date_start[2].astype(int) * 1)\n",
    "\n",
    "    w_events_date_end = extreme_w_events['EndTime(UTC)'].str.split(' ', expand=True)\n",
    "    w_events_date_end = w_events_date_end[0].str.split('-', expand=True)\n",
    "    extreme_w_events['event_end_dt'] = (w_events_date_end[0].astype(int) * 10000) + (w_events_date_end[1].astype(int) * 100) + (w_events_date_end[2].astype(int) * 1)\n",
    "    \n",
    "    return extreme_w_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6274206, 13) (1333526, 13)\n",
      "extreme events represent about 21.25 percent of all events in the original data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "extreme_w_events = load_weatherEvents(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety Events Data\n",
    "One may load and work with safety events dataset using the code below, which includes added 'coordinateID' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_safetyEvents(wd):\n",
    "    '''\n",
    "    not sure if we will use the safety events dataset or not\n",
    "    it is included here with incident date and coordinate ID so we can join on other datasets\n",
    "\n",
    "    '''\n",
    "\n",
    "    # ingest data\n",
    "    m_safety_events = pd.read_csv('Major_Safety_Events.csv')\n",
    "\n",
    "    # drop events without an area\n",
    "    m_safety_events.dropna(subset=['Primary UZA Name'],inplace=True)\n",
    "\n",
    "    # add city, state to each event\n",
    "    safety_events_citystate = m_safety_events['Primary UZA Name'].str.split(',', expand=True)\n",
    "    safety_events_state = safety_events_citystate[1].str.split('-', expand=True)\n",
    "    safety_events_state[0] = safety_events_state[0].str.strip()\n",
    "    m_safety_events['City'] = safety_events_citystate[0]\n",
    "    m_safety_events['State'] = safety_events_state[0]\n",
    "\n",
    "    # add coordinate ID by merging with cities data\n",
    "    m_safety_events = m_safety_events.merge(cities_df, how='inner', left_on=['City','State'], right_on=['CITY','STATE_CODE'])\n",
    "    m_safety_events['coordinateID'] = list(zip(round(m_safety_events['LATITUDE'],1),round(m_safety_events['LONGITUDE'],1)))\n",
    "\n",
    "    # add event date in integer format yyyymmdd\n",
    "    safety_events_date = m_safety_events['Incident Date'].str.split(' ', expand=True)\n",
    "    safety_events_date = safety_events_date[0].str.split('/', expand=True)\n",
    "    m_safety_events['event_date'] = (safety_events_date[2].astype(int) * 10000) + (safety_events_date[1].astype(int) * 100) + (safety_events_date[0].astype(int) * 1)\n",
    "\n",
    "    # filter for relevant events\n",
    "    inscope_events = ['Non-Rail Collision', 'Main Line Derailment', 'Rail Fire', 'Rail Collision', 'Flood','Ferry Boat Collision','Other High Winds','Tornado','Lightning','Hurricane']\n",
    "    inscope2_events = ['Non-Rail Collision', 'Main Line Derailment', 'Rail Fire', 'Rail Collision']\n",
    "\n",
    "    rail_events = m_safety_events[m_safety_events['Event Type'].isin(inscope_events)]\n",
    "    rail_events2 = m_safety_events[m_safety_events['Event Type'].isin(inscope2_events)]\n",
    "\n",
    "    print(m_safety_events.shape, rail_events.shape)\n",
    "    print(f'rail events represent about {round(100*(rail_events.shape[0]/m_safety_events.shape[0]),2)} percent of all events in the original data')\n",
    "\n",
    "    print(m_safety_events.shape, rail_events2.shape)\n",
    "    print(f'rail events2 represent about {round(100*(rail_events2.shape[0]/m_safety_events.shape[0]),2)} percent of all events in the original data')\n",
    "\n",
    "    # based on this data i recommend we use inscope2_events, if anything\n",
    "     # can add more fields, but we need to be careful managing memory when we merge with other data\n",
    "    rail_events2 = rail_events2[['coordinateID', 'event_date', 'Incident Number', 'Event Type']]\n",
    "    return rail_events2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DtypeWarning: Columns (1,32,33,36,39,40) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54007, 97) (43696, 97)\n",
      "rail events represent about 80.91 percent of all events in the original data\n",
      "(54007, 97) (43653, 97)\n",
      "rail events2 represent about 80.83 percent of all events in the original data\n"
     ]
    }
   ],
   "source": [
    "rail_events = load_safetyEvents(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## NOAA Daily Observation Data\n",
    "dataset overview:  \n",
    "https://www.ncei.noaa.gov/metadata/geoportal/rest/metadata/item/gov.noaa.ncdc:C00861/html  \n",
    "main ftp directory:  \n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/  \n",
    "readme:  \n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt  \n",
    "required ftp files:  \n",
    "1. ghcnd-stations.txt\n",
    "2. ghcnd-states.txt\n",
    "3. ghcnd_all.tar.gz - YOU WILL NEED TO UNZIP THIS \n",
    "\n",
    "\n",
    "## Highway Rail Grade Crossing Accident Data\n",
    "dataset overview:  \n",
    "https://data.transportation.gov/Railroads/Highway-Rail-Grade-Crossing-Accident-Data/7wn6-i5b9  \n",
    "download link:  \n",
    "https://data.transportation.gov/api/views/7wn6-i5b9/rows.csv?accessType=DOWNLOAD&bom=true&format=true  \n",
    "from the overview page, click export -> choose your output type (I chose CSV for this code).  or use the download link\n",
    "\n",
    "\n",
    "## Weather Events 2016 - 2020\n",
    "dataset overview:  \n",
    "https://www.kaggle.com/sobhanmoosavi/us-weather-events  \n",
    "download link:  \n",
    "https://www.kaggle.com/sobhanmoosavi/us-weather-events/download\n",
    "\n",
    "\n",
    "## Major Safety Events\n",
    "dataset overview:  \n",
    "https://www.transit.dot.gov/ntd/data-product/safety-security-major-only-time-series-data  \n",
    "dataset download:  \n",
    "https://data.transportation.gov/Public-Transit/Major-Safety-Events/9ivb-8ae9\n",
    "\n",
    "\n",
    "# US Cities\n",
    "dataset overview:  \n",
    "https://github.com/kelvins/US-Cities-Database  \n",
    "dataset download:  \n",
    "https://github.com/kelvins/US-Cities-Database/blob/main/csv/us_cities.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
