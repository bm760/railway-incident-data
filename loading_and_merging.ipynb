{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Datasets\n",
    "\n",
    "\n",
    "## NOAA Daily Observation Data\n",
    "dataset overview:\n",
    "\n",
    "https://www.ncei.noaa.gov/metadata/geoportal/rest/metadata/item/gov.noaa.ncdc:C00861/html\n",
    "\n",
    "main ftp directory:\n",
    "\n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/\n",
    "\n",
    "readme:\n",
    "\n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt\n",
    "\n",
    "required ftp files:\n",
    "1. ghcnd-stations.txt\n",
    "2. ghcnd-states.txt\n",
    "3. ghcnd_ghc.tar.gz - YOU WILL NEED TO UNZIP THIS \n",
    "\n",
    "\n",
    "## Highway Rail Grade Crossing Accident Data\n",
    "dataset overview:\n",
    "\n",
    "https://data.transportation.gov/Railroads/Highway-Rail-Grade-Crossing-Accident-Data/7wn6-i5b9\n",
    "\n",
    "download link:\n",
    "\n",
    "https://data.transportation.gov/api/views/7wn6-i5b9/rows.csv?accessType=DOWNLOAD&bom=true&format=true\n",
    "\n",
    "from the overview page, click export -> choose your output type (I chose CSV for this code).  or use the download link\n",
    "\n",
    "\n",
    "## Weather Events 2016 - 2020\n",
    "dataset overview:\n",
    "\n",
    "https://www.kaggle.com/sobhanmoosavi/us-weather-events\n",
    "\n",
    "download link:\n",
    "\n",
    "https://www.kaggle.com/sobhanmoosavi/us-weather-events/download\n",
    "\n",
    "\n",
    "## Major Safety Events\n",
    "\n",
    "dataset overview:\n",
    "- link missing \n",
    "\n",
    "dataset download:\n",
    "- link missing\n",
    "(we have it stored in drive but I dont know the original source)\n",
    "\n",
    "https://drive.google.com/file/d/1eMuGXTA9W2YRJydmn6ff2MXZup7JJBfF/view?usp=sharing\n",
    "\n",
    "\n",
    "# US Cities\n",
    "\n",
    "dataset overview:\n",
    "\n",
    "https://github.com/kelvins/US-Cities-Database\n",
    "\n",
    "dataset download:\n",
    "\n",
    "https://github.com/kelvins/US-Cities-Database/blob/main/csv/us_cities.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Station Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this dataset contains weather stations and their locations published by the noaa \n",
    "set is used to map daily weather observations against rail crossings\n",
    "'''\n",
    "\n",
    "# gather weather station metadata from the stations.txt file\n",
    "\n",
    "# you will need to change the path to where your txt file resides\n",
    "f = open(\"F:\\weather\\ghcnd-stations.txt\",\"r\")\n",
    "lines = f.readlines()\n",
    "\n",
    "# columns in the station file\n",
    "colnames = ['ID', 'LAT', 'LON', 'ELEV', 'STATE', 'NAME', 'GSN', 'HCNCRN', 'WMOID']\n",
    "stationlist = []\n",
    "\n",
    "# initialize dataframe with correct columns\n",
    "stations = pd.DataFrame(columns=colnames)\n",
    "\n",
    "# iterate through stations and add them to our collection of stations if they are in the US\n",
    "for line in lines:\n",
    "    # first 2 characters are the country code , we only care about us stations\n",
    "    if line[0:2] == 'US':\n",
    "        \n",
    "        # the description of the file seemed slightly off, i tested and found these column numbers to work best\n",
    "        row = {\"ID\": line[0:11].upper(),\n",
    "                \"LAT\": float(line[13:20]),\n",
    "                \"LON\": float(line[21:30]),\n",
    "                \"ELEV\": float(line[31:37]),\n",
    "                \"STATE\": line[38:40],\n",
    "                \"NAME\": line[41:71],\n",
    "                \"GSN\": line[72:75],\n",
    "                \"HCNCRN\": line[76:79],\n",
    "                \"WMOID\": line[80:85]\n",
    "               }\n",
    "        stationlist.append(row)\n",
    "    else:\n",
    "        pass\n",
    "stations = stations.append(stationlist)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# read in states dataset to supplement weather stations data\n",
    "fstates = open(\"F:\\weather\\ghcnd-states.txt\",\"r\")\n",
    "lines3 = fstates.readlines()\n",
    "\n",
    "colnames = ['CODE', 'NAME']\n",
    "\n",
    "# create dataframe of state data\n",
    "states = pd.DataFrame(columns=colnames)\n",
    "for line in lines3:\n",
    "    modline = line.strip('\\n')\n",
    "    data = {'CODE': line[0:2],\n",
    "            \"NAME\": modline[3:50]\n",
    "           }\n",
    "    states = states.append(data, ignore_index=True)    \n",
    "\n",
    "fstates.close()\n",
    "\n",
    "# add state data to the stations dataset\n",
    "stationplus = stations.join(states.set_index('CODE'), on='STATE', rsuffix='_STATE')\n",
    "\n",
    "# create our key feature: coordinateID (wcoordinateID for weather)\n",
    "# round latitude & longitude to 1 decimal, combine them in a tuple (lat, lon)\n",
    "stationplus['wcoordinateID'] = list(zip(round(stationplus['LAT'],1),round(stationplus['LON'],1)))\n",
    "stationplus = stationplus[['ID','ELEV','wcoordinateID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>ELEV</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN</th>\n",
       "      <th>HCNCRN</th>\n",
       "      <th>WMOID</th>\n",
       "      <th>NAME_STATE</th>\n",
       "      <th>wcoordinateID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>US009052008</td>\n",
       "      <td>43.7333</td>\n",
       "      <td>-96.6333</td>\n",
       "      <td>482.0</td>\n",
       "      <td>SD</td>\n",
       "      <td>SIOUX FALLS (ENVIRON. CANADA)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SOUTH DAKOTA</td>\n",
       "      <td>(43.7, -96.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>US10RMHS145</td>\n",
       "      <td>40.5268</td>\n",
       "      <td>-105.1113</td>\n",
       "      <td>1569.1</td>\n",
       "      <td>CO</td>\n",
       "      <td>RMHS 1.6 SSW</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>(40.5, -105.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>US10ADAM001</td>\n",
       "      <td>40.5680</td>\n",
       "      <td>-98.5069</td>\n",
       "      <td>598.0</td>\n",
       "      <td>NE</td>\n",
       "      <td>JUNIATA 1.5 S</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>(40.6, -98.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>US10ADAM002</td>\n",
       "      <td>40.5093</td>\n",
       "      <td>-98.5493</td>\n",
       "      <td>601.1</td>\n",
       "      <td>NE</td>\n",
       "      <td>JUNIATA 6.0 SSW</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>(40.5, -98.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>US10ADAM003</td>\n",
       "      <td>40.4663</td>\n",
       "      <td>-98.6537</td>\n",
       "      <td>615.1</td>\n",
       "      <td>NE</td>\n",
       "      <td>HOLSTEIN 0.1 NW</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>(40.5, -98.7)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID      LAT       LON    ELEV STATE  \\\n",
       "0  US009052008  43.7333  -96.6333   482.0    SD   \n",
       "1  US10RMHS145  40.5268 -105.1113  1569.1    CO   \n",
       "2  US10ADAM001  40.5680  -98.5069   598.0    NE   \n",
       "3  US10ADAM002  40.5093  -98.5493   601.1    NE   \n",
       "4  US10ADAM003  40.4663  -98.6537   615.1    NE   \n",
       "\n",
       "                             NAME  GSN HCNCRN  WMOID    NAME_STATE  \\\n",
       "0  SIOUX FALLS (ENVIRON. CANADA)                      SOUTH DAKOTA   \n",
       "1  RMHS 1.6 SSW                                           COLORADO   \n",
       "2  JUNIATA 1.5 S                                          NEBRASKA   \n",
       "3  JUNIATA 6.0 SSW                                        NEBRASKA   \n",
       "4  HOLSTEIN 0.1 NW                                        NEBRASKA   \n",
       "\n",
       "    wcoordinateID  \n",
       "0   (43.7, -96.6)  \n",
       "1  (40.5, -105.1)  \n",
       "2   (40.6, -98.5)  \n",
       "3   (40.5, -98.5)  \n",
       "4   (40.5, -98.7)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Cities Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this data will be used to attach coordinateID \n",
    "to other datasets that only have city or county level data\n",
    "'''\n",
    "\n",
    "\n",
    "cities = pd.read_csv('us_cities.csv')\n",
    "\n",
    "# standardize county and state, city is not populated for all events.\n",
    "# one change to approach would be to include all cities + the grouped mean of each county\n",
    "cities['County'] = cities['COUNTY'].str.upper()\n",
    "cities['State'] = cities['STATE_NAME'].str.upper()\n",
    "\n",
    "# subset of data that we care about, lat+lon to make coordinateID, county, state, state code to merge on\n",
    "counties = cities[['County','State','LATITUDE','LONGITUDE','STATE_CODE']]\n",
    "grouped_counties = counties.groupby(['State','County'])\n",
    "grouped_meancounties = grouped_counties.mean()\n",
    "grouped_meancounties = grouped_meancounties.reset_index()\n",
    "grouped_meancounties['wcoordinateID'] = list(zip(round(grouped_meancounties['LATITUDE'],1),round(grouped_meancounties['LONGITUDE'],1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rail Crossing Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231891, 10)\n",
      "(96411, 5)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "this dataset will be used to provide instances for our model to learn from.\n",
    "we also use this dataset to limit the number of weather stations we include  observations from\n",
    "'''\n",
    "\n",
    "# ingest data\n",
    "railcrossing = pd.read_csv('Highway-Rail_Grade_Crossing_Accident_Data.csv')\n",
    "\n",
    "\n",
    "# gather the fields necessary for coordinateID, as well as any  fields you want for analysis later \n",
    "# change to approache what fields we include in refined_rr\n",
    "refined_rr = railcrossing[['Incident Number','Date','County Name', 'State Name']]\n",
    "\n",
    "# drop any incident without a date\n",
    "refined_rr = refined_rr.dropna(subset=['Date'])\n",
    "\n",
    "# create our feature incident date, which is an integer with format: yyyymmdd \n",
    "incident_date = refined_rr['Date'].str.split(' ', expand=True)\n",
    "incident_date = incident_date[0].str.split('/', expand=True)\n",
    "refined_rr['incident_date'] = (incident_date[2].astype(int) * 10000) + (incident_date[0].astype(int) * 100) + (incident_date[1].astype(int) * 1)\n",
    "\n",
    "# merge accident data with city/county data to add coordinateID to each accident.\n",
    "merg_rail_city = refined_rr.merge(grouped_meancounties, how='inner', left_on=['County Name','State Name'], right_on=['County','State'])\n",
    "print(merg_rail_city.shape)\n",
    "merg_rail_city = merg_rail_city[merg_rail_city['incident_date'] > 19900000]\n",
    "merg_rail_city = merg_rail_city[['Incident Number', 'incident_date', 'wcoordinateID', 'State', 'County']]\n",
    "print(merg_rail_city.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Incident Number</th>\n",
       "      <th>incident_date</th>\n",
       "      <th>wcoordinateID</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CA0109200</td>\n",
       "      <td>20090114</td>\n",
       "      <td>(36.7, -119.7)</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>FRESNO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CA0208200</td>\n",
       "      <td>20080209</td>\n",
       "      <td>(36.7, -119.7)</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>FRESNO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>104019</td>\n",
       "      <td>20070404</td>\n",
       "      <td>(36.7, -119.7)</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>FRESNO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>104074</td>\n",
       "      <td>20070410</td>\n",
       "      <td>(36.7, -119.7)</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>FRESNO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>103616</td>\n",
       "      <td>20070221</td>\n",
       "      <td>(36.7, -119.7)</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>FRESNO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Incident Number  incident_date   wcoordinateID       State  County\n",
       "0       CA0109200       20090114  (36.7, -119.7)  CALIFORNIA  FRESNO\n",
       "1       CA0208200       20080209  (36.7, -119.7)  CALIFORNIA  FRESNO\n",
       "2          104019       20070404  (36.7, -119.7)  CALIFORNIA  FRESNO\n",
       "3          104074       20070410  (36.7, -119.7)  CALIFORNIA  FRESNO\n",
       "4          103616       20070221  (36.7, -119.7)  CALIFORNIA  FRESNO"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merg_rail_city.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constricting Weather Station Inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of weather stations - only those that share coordinateID with an accident\n",
    "\n",
    "merged_stations_accidents = stationplus.merge(merg_rail_city,left_on='wcoordinateID', right_on='wcoordinateID', how='inner')\n",
    "\n",
    "# here we have a list of the station IDs that were included in the merged dataframe\n",
    "incident_stations = [x.upper() for x in merged_stations_accidents['ID'].unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Daily Observations Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "prior attempts to include this data failed because ghcnd_ghc.tar.gz is too large.\n",
    "by limiting the number of stations included to only those where incidents occurred,\n",
    "and by limiting the observation years from each station, we can reduce the amount of\n",
    "memory required to process this data\n",
    "\n",
    "'''\n",
    "# with assistance from \n",
    "# https://stackoverflow.com/questions/62165172/convert-dly-files-to-csv-using-python\n",
    "# fields as given by the spec\n",
    "\n",
    "fields = [\n",
    "    [\"ID\", 1, 11],\n",
    "    [\"YEAR\", 12, 15],\n",
    "    [\"MONTH\", 16, 17],\n",
    "    [\"ELEMENT\", 18, 21]]\n",
    "\n",
    "offset = 22\n",
    "\n",
    "for value in range(1, 32):\n",
    "    fields.append((f\"VALUE{value}\", offset,     offset + 4))\n",
    "    fields.append((f\"MFLAG{value}\", offset + 5, offset + 5))\n",
    "    fields.append((f\"QFLAG{value}\", offset + 6, offset + 6))\n",
    "    fields.append((f\"SFLAG{value}\", offset + 7, offset + 7))\n",
    "    offset += 8\n",
    "\n",
    "# Modify fields to use Python numbering\n",
    "fields = [[var, start - 1, end] for var, start, end in fields]\n",
    "fieldnames = [var for var, start, end in fields]\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# the goal of this code is to make 1 file TOTAL from many (1 per station), which can then be filtered  by year. \n",
    "\n",
    "# enter where you want a csv saved - it will be many Gigs\n",
    "csv_filename = cwd+'\\\\noaa_relevant_dailies.csv'\n",
    "\n",
    "with open(csv_filename, 'w', newline='') as f_csv:\n",
    "    \n",
    "    # glob.glob should aim to where you unzipped the ghcnd_ghc.tar.gz , inside that will be ghcnd_hcn folder, which contains all the dailies\n",
    "    for dly_filename in glob.glob(r'F:\\weather\\hcn\\ghcnd_hcn\\*.dly', recursive=True):\n",
    "        path, name = os.path.split(dly_filename)\n",
    "        station = name[:-4].upper()\n",
    "        if station in incident_stations:\n",
    "            # you could replace this with adding to a dataframe or something else, but i am running out of brain power.\n",
    "            with open(dly_filename, newline='') as f_dly:\n",
    "                spamwriter  = csv.writer(f_csv)\n",
    "                spamwriter.writerow(fieldnames) \n",
    "\n",
    "                for line in f_dly:\n",
    "                    row = [line[start:end].strip() for var, start, end in fields]\n",
    "                    year = int(row[1])\n",
    "                    \n",
    "                    # important check to save memory, only add recent observations\n",
    "                    if year > 1990:\n",
    "                        spamwriter.writerow(row)\n",
    "\n",
    "            \n",
    "                \n",
    "                \n",
    "# end product is a csv with us weather station data.  needs more cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Daily Data Wrangling Pt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (1,2,4,6,8,11,12,16,19,20,23,24,28,31,32,35,36,40,43,44,47,48,51,52,55,56,59,60,63,64,67,68,71,72,75,76,79,80,83,84,88,92,95,96,100,104,107,108,111,112,116,120,123,124,127) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10880411, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('noaa_relevant_dailies.csv')\n",
    "# we added a header row for every file, but we only need 1 header row. remove the others:\n",
    "df = df[df['YEAR'] != 'YEAR']\n",
    "\n",
    "# month and year had some strings and some ints. lets standardize\n",
    "df['YEAR'] = pd.to_numeric(df['YEAR'])\n",
    "df['MONTH'] = pd.to_numeric(df['MONTH'])\n",
    "\n",
    "\n",
    "# base for transposed data\n",
    "base = pd.DataFrame(columns=['ID','YEAR','MONTH','ELEMENT','VALUE', 'MFLAG', 'QFLAG', 'SFLAG'])\n",
    "\n",
    "# loop through all days to partially transpose the file (day cols -> rows)\n",
    "for i in range(1,32):\n",
    "    colnames = [f'VALUE{i}', f'MFLAG{i}', f'QFLAG{i}', f'SFLAG{i}']\n",
    "    newcolnames = ['VALUE', 'MFLAG', 'QFLAG', 'SFLAG']\n",
    "    col_order = ['ID','YEAR','MONTH','DAY','ELEMENT', colnames[0], colnames[1], colnames[2], colnames[3]]\n",
    "\n",
    "    df_new = df[['ID','YEAR','MONTH','ELEMENT', colnames[0], colnames[1], colnames[2], colnames[3]]]\n",
    "    df_new['DAY'] = i\n",
    "    df_new = df_new[col_order]\n",
    "    df_new = df_new.rename(columns={colnames[0]:newcolnames[0], colnames[1]:newcolnames[1], colnames[2]:newcolnames[2], colnames[3]:newcolnames[3]})\n",
    "    base = pd.concat([base, df_new], sort=False)\n",
    "\n",
    "\n",
    "newcsv = base[['ID','YEAR','MONTH','DAY','ELEMENT','VALUE','MFLAG','QFLAG','SFLAG']]\n",
    "\n",
    "daily_station_coordinates = stationplus[['ID','wcoordinateID']]\n",
    "daily_final = newcsv.merge(daily_station_coordinates, left_on='ID', right_on='ID')\n",
    "daily_final.to_csv('final_daily_observations.csv')\n",
    "daily_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety Events Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54007, 97) (43696, 97)\n",
      "rail events represent about 80.91 percent of all events in the original data\n",
      "(54007, 97) (43653, 97)\n",
      "rail events2 represent about 80.83 percent of all events in the original data\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "not sure if we will use the safety events dataset or not\n",
    "it is included here with incident date and coordinate ID so we can join on other datasets\n",
    "\n",
    "'''\n",
    "\n",
    "# ingest data\n",
    "m_safety_events = pd.read_csv('Major_Safety_Events.csv')\n",
    "\n",
    "# drop events without an area\n",
    "m_safety_events.dropna(subset=['Primary UZA Name'],inplace=True)\n",
    "\n",
    "# add city, state to each event\n",
    "safety_events_citystate = m_safety_events['Primary UZA Name'].str.split(',', expand=True)\n",
    "safety_events_state = safety_events_citystate[1].str.split('-', expand=True)\n",
    "safety_events_state[0] = safety_events_state[0].str.strip()\n",
    "m_safety_events['City'] = safety_events_citystate[0]\n",
    "m_safety_events['State'] = safety_events_state[0]\n",
    "\n",
    "# add coordinate ID by merging with cities data\n",
    "m_safety_events = m_safety_events.merge(cities, how='inner', left_on=['City','State'], right_on=['CITY','STATE_CODE'])\n",
    "m_safety_events['coordinateID'] = list(zip(round(m_safety_events['LATITUDE'],1),round(m_safety_events['LONGITUDE'],1)))\n",
    "\n",
    "# add event date in integer format yyyymmdd\n",
    "safety_events_date = m_safety_events['Incident Date'].str.split(' ', expand=True)\n",
    "safety_events_date = safety_events_date[0].str.split('/', expand=True)\n",
    "m_safety_events['event_date'] = (safety_events_date[2].astype(int) * 10000) + (safety_events_date[1].astype(int) * 100) + (safety_events_date[0].astype(int) * 1)\n",
    "\n",
    "# filter for relevant events\n",
    "inscope_events = ['Non-Rail Collision', 'Main Line Derailment', 'Rail Fire', 'Rail Collision', 'Flood','Ferry Boat Collision','Other High Winds','Tornado','Lightning','Hurricane']\n",
    "inscope2_events = ['Non-Rail Collision', 'Main Line Derailment', 'Rail Fire', 'Rail Collision']\n",
    "\n",
    "rail_events = m_safety_events[m_safety_events['Event Type'].isin(inscope_events)]\n",
    "rail_events2 = m_safety_events[m_safety_events['Event Type'].isin(inscope2_events)]\n",
    "\n",
    "print(m_safety_events.shape, rail_events.shape)\n",
    "print(f'rail events represent about {round(100*(rail_events.shape[0]/m_safety_events.shape[0]),2)} percent of all events in the original data')\n",
    "\n",
    "print(m_safety_events.shape, rail_events2.shape)\n",
    "print(f'rail events2 represent about {round(100*(rail_events2.shape[0]/m_safety_events.shape[0]),2)} percent of all events in the original data')\n",
    "\n",
    "# based on this data i recommend we use inscope2_events, if anything\n",
    " # can add more fields, but we need to be careful managing memory when we merge with other data\n",
    "rail_events2 = rail_events2[['coordinateID', 'event_date', 'Incident Number', 'Event Type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Events Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6274206, 13) (1333526, 13)\n",
      "extreme events represent about 21.25 percent of all events in the original data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Skurai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This data may not actually be used by us if we like the daily data\n",
    "or possibly we will use both somehow\n",
    "'''\n",
    "w_events = pd.read_csv('WeatherEvents_Jan2016-Dec2020.csv')\n",
    "\n",
    "# it looks like 'Severe' and 'Heavy' are most extreme, so filter to these \n",
    "extreme_severities = ['Severe', 'Heavy']\n",
    "extreme_w_events = w_events[w_events['Severity'].isin(extreme_severities)]\n",
    "print(w_events.shape, extreme_w_events.shape)\n",
    "print(f'extreme events represent about {round(100*(extreme_w_events.shape[0]/w_events.shape[0]),2)} percent of all events in the original data')\n",
    "\n",
    "# add coordinateID\n",
    "extreme_w_events['wcoordinateID'] = list(zip(round(extreme_w_events['LocationLat'],1),round(extreme_w_events['LocationLng'],1)))\n",
    "\n",
    "# calculate start and end dates as integers yyyymmdd\n",
    "w_events_date_start = extreme_w_events['StartTime(UTC)'].str.split(' ', expand=True)\n",
    "w_events_date_start = w_events_date_start[0].str.split('-', expand=True)\n",
    "extreme_w_events['event_start_dt'] = (w_events_date_start[0].astype(int) * 10000) + (w_events_date_start[1].astype(int) * 100) + (w_events_date_start[2].astype(int) * 1)\n",
    "\n",
    "w_events_date_end = extreme_w_events['EndTime(UTC)'].str.split(' ', expand=True)\n",
    "w_events_date_end = w_events_date_end[0].str.split('-', expand=True)\n",
    "extreme_w_events['event_end_dt'] = (w_events_date_end[0].astype(int) * 10000) + (w_events_date_end[1].astype(int) * 100) + (w_events_date_end[2].astype(int) * 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Safety Events on Weather Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "if we do include weather events data + safety events data\n",
    "this is the code to merge them \n",
    "'''\n",
    "\n",
    "# merge dataframes\n",
    "safetyweather_merged = extreme_w_events.merge(rail_events2,left_on='wcoordinateID',right_on='coordinateID')\n",
    "\n",
    "# number of days after event end date that we continue to look for incidents\n",
    "lag_days = 2\n",
    "\n",
    "# filter our dataframe for observations where weather event was near the safety event\n",
    "final_safetyweather_merged = safetyweather_merged[(safetyweather_merged['event_date'] >= safetyweather_merged['event_start_dt']) & (safetyweather_merged['event_date'] <= safetyweather_merged['event_end_dt'] + lag_days)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Rail Accidents on Weather Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "if we do include weather events data + railroad events data\n",
    "this is the code to merge them \n",
    "'''\n",
    "\n",
    "# merge dataframes\n",
    "railweather_merged = extreme_w_events.merge(merg_rail_city,left_on='wcoordinateID',right_on='wcoordinateID')\n",
    "\n",
    "# number of days after event end date that we continue to look for incidents\n",
    "lag_days = 2\n",
    "\n",
    "# filter our dataframe for observations where weather event was near the rail  event\n",
    "final_railweather_merged = railweather_merged[(railweather_merged['incident_date'] >= railweather_merged['event_start_dt']) & (railweather_merged['incident_date'] <= railweather_merged['event_end_dt'] + lag_days)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Rail Accidents on Daily Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8482a3a0991d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# another memory error here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mraildaily_merged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdaily_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerg_rail_city\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wcoordinateID'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wcoordinateID'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   7332\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7333\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7334\u001b[1;33m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7335\u001b[0m         )\n\u001b[0;32m   7336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indicator_pre_merge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m         \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[0mldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    857\u001b[0m             )\n\u001b[0;32m    858\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    836\u001b[0m         \u001b[1;34m\"\"\" return the join indexers \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m         return _get_join_indexers(\n\u001b[1;32m--> 838\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m         )\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m     \u001b[0mjoin_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_join_functions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mjoin_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\join.pyx\u001b[0m in \u001b[0;36mpandas._libs.join.inner_join\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\join.pyx\u001b[0m in \u001b[0;36mpandas._libs.join._get_result_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[0;32m   1712\u001b[0m             \u001b[1;31m# (s.t. df.values is c-contiguous and df._data.blocks[0] is its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1713\u001b[0m             \u001b[1;31m# f-contiguous transpose)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1714\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1715\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "the most likely candidate for us to use for our model\n",
    "'''\n",
    "\n",
    "# memory error here, we have to reduce somewhere further up in the process....\n",
    "raildaily_merged = daily_final.merge(merg_rail_city,left_on='wcoordinateID',right_on='wcoordinateID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
